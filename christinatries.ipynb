{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2735c935",
   "metadata": {
    "id": "2735c935"
   },
   "source": [
    "<h1><center> Group Assignment: Bird Classifier  </center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cd0b3ce3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 250
    },
    "executionInfo": {
     "elapsed": 224,
     "status": "error",
     "timestamp": 1694776892897,
     "user": {
      "displayName": "York Pudds (Yorkie)",
      "userId": "15802187727162843994"
     },
     "user_tz": -120
    },
    "id": "cd0b3ce3",
    "outputId": "da1cfd65-38c1-4971-dd05-c2a17ce22140"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS backend is available. Running on Apple GPU.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    print(\"MPS backend is available. Running on Apple GPU.\")\n",
    "else:\n",
    "    print(\"MPS backend is not available. Running on CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "60e54e04",
   "metadata": {
    "id": "60e54e04",
    "outputId": "2dda9a07-4c8b-4e52-e8a0-744f58857013"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor on MPS: tensor([1., 2., 3.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\")  # Use MPS for Apple Silicon\n",
    "x = torch.tensor([1.0, 2.0, 3.0], device=device)\n",
    "print(\"Tensor on MPS:\", x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4105b4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attribute Map Sample: [(1, 'has_bill_shape::curved_(up_or_down)'), (2, 'has_bill_shape::dagger'), (3, 'has_bill_shape::hooked'), (4, 'has_bill_shape::needle'), (5, 'has_bill_shape::hooked_seabird'), (6, 'has_bill_shape::spatulate'), (7, 'has_bill_shape::all-purpose'), (8, 'has_bill_shape::cone'), (9, 'has_bill_shape::specialized'), (10, 'has_wing_color::blue')]\n"
     ]
    }
   ],
   "source": [
    "# Parse the attributes.txt file\n",
    "attribute_map = {}\n",
    "with open('data/attributes.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(' ', 1)  # Split at the first space\n",
    "        if len(parts) == 2:\n",
    "            attr_id, attr_desc = parts\n",
    "            attribute_map[int(attr_id)] = attr_desc\n",
    "\n",
    "# Example: Print a few mappings\n",
    "print(\"Attribute Map Sample:\", list(attribute_map.items())[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ab30c98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attributes Array Shape: (200, 312)\n",
      "Sample Attributes: [[0.0106384  0.0106384  0.00709227 ... 0.00918617 0.02526198 0.02066889]\n",
      " [0.         0.01133243 0.00944369 ... 0.00266542 0.02132333 0.05863916]\n",
      " [0.         0.         0.00742474 ... 0.         0.00885258 0.01770516]\n",
      " [0.         0.         0.00386105 ... 0.00683957 0.03647772 0.04331729]\n",
      " [0.         0.03508838 0.         ... 0.0027513  0.01513216 0.15819981]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the .npy file\n",
    "attributes_array = np.load('data/attributes.npy')\n",
    "\n",
    "# Example: Print the shape and a sample\n",
    "print(\"Attributes Array Shape:\", attributes_array.shape)\n",
    "print(\"Sample Attributes:\", attributes_array[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6b55dceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    data/train_images/1.jpg\n",
      "1    data/train_images/2.jpg\n",
      "2    data/train_images/3.jpg\n",
      "3    data/train_images/4.jpg\n",
      "4    data/train_images/5.jpg\n",
      "Name: image_path, dtype: object\n",
      "data/train_images/1.jpg exists: True\n",
      "data/train_images/2.jpg exists: True\n",
      "data/train_images/3.jpg exists: True\n",
      "data/train_images/4.jpg exists: True\n",
      "data/train_images/5.jpg exists: True\n",
      "[  0   0   0 ... 199 199 199]\n",
      "                image_path  label    attr_0    attr_1    attr_2    attr_3  \\\n",
      "0  data/train_images/1.jpg      0  0.010638  0.010638  0.007092  0.003546   \n",
      "1  data/train_images/2.jpg      0  0.000000  0.011332  0.009444  0.000000   \n",
      "2  data/train_images/3.jpg      0  0.000000  0.000000  0.007425  0.000000   \n",
      "3  data/train_images/4.jpg      0  0.000000  0.000000  0.003861  0.000000   \n",
      "4  data/train_images/5.jpg      0  0.000000  0.035088  0.000000  0.000000   \n",
      "\n",
      "     attr_4    attr_5    attr_6    attr_7  ...  attr_302  attr_303  attr_304  \\\n",
      "0  0.138299  0.065603  0.000000  0.005319  ...  0.000000  0.005439  0.005439   \n",
      "1  0.202095  0.041552  0.015110  0.005666  ...  0.006291  0.000000  0.111144   \n",
      "2  0.002475  0.000000  0.000000  0.074247  ...  0.000000  0.000000  0.190411   \n",
      "3  0.003861  0.013514  0.005792  0.073360  ...  0.004885  0.000000  0.190531   \n",
      "4  0.000000  0.000000  0.102458  0.070177  ...  0.000000  0.000000  0.204036   \n",
      "\n",
      "   attr_305  attr_306  attr_307  attr_308  attr_309  attr_310  attr_311  \n",
      "0  0.228446  0.000000  0.000000  0.186020  0.009186  0.025262  0.020669  \n",
      "1  0.008388  0.000000  0.046135  0.202572  0.002665  0.021323  0.058639  \n",
      "2  0.012555  0.000000  0.010462  0.203609  0.000000  0.008853  0.017705  \n",
      "3  0.000000  0.000000  0.000000  0.152750  0.006840  0.036478  0.043317  \n",
      "4  0.002458  0.002458  0.000000  0.031640  0.002751  0.015132  0.158200  \n",
      "\n",
      "[5 rows x 314 columns]\n",
      "Encoded Attributes Shape: (3926, 36427)\n",
      "Reduced Attributes Shape: (3926, 128)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torch import nn, optim\n",
    "from PIL import Image\n",
    "\n",
    "# Step 1: Read and preprocess the CSV\n",
    "data = pd.read_csv('data/train_images.csv')\n",
    "\n",
    "# Prepend 'data' to paths if not already present\n",
    "data['image_path'] = data['image_path'].apply(lambda x: f\"data{x}\" if not x.startswith(\"data\") else x)\n",
    "\n",
    "# Verify the first few paths\n",
    "print(data['image_path'].head())\n",
    "\n",
    "# Check if files exist\n",
    "for path in data['image_path'].head():\n",
    "    print(f\"{path} exists: {os.path.exists(path)}\")\n",
    "\n",
    "# Step 2: Define a PyTorch Dataset \n",
    "# import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "\n",
    "# Custom Dataset\n",
    "class CombinedDataset(Dataset):\n",
    "    def __init__(self, image_paths, attributes, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.attributes = torch.tensor(attributes, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx])\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        attribute = self.attributes[idx]\n",
    "        label = self.labels[idx]\n",
    "        return image, attribute, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "# Create the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Define transformations\n",
    "from torchvision import transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.Resize((150, 150)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) \n",
    "])\n",
    "\n",
    "# Create dataset and dataloader\n",
    "image_paths = data['image_path'].values\n",
    "labels = data['label'].values - 1  # Ensure labels are zero-indexed (0 to 199)\n",
    "print(labels)\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Assuming 'image_paths' and 'labels' from your dataset\n",
    "data_df = pd.DataFrame({\n",
    "    'image_path': image_paths,\n",
    "    'label': labels\n",
    "})\n",
    "\n",
    "# Add attributes to the DataFrame\n",
    "attribute_df = pd.DataFrame(attributes_array, columns=[f\"attr_{i}\" for i in range(attributes_array.shape[1])])\n",
    "data_df = pd.concat([data_df, attribute_df], axis=1)\n",
    "\n",
    "# Example: Check combined data\n",
    "print(data_df.head())\n",
    "\n",
    "# One-hot encode categorical attributes\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "encoded_attributes = encoder.fit_transform(data_df.iloc[:, 2:].values)  # Assuming attributes start at column 2\n",
    "\n",
    "# Example: Print the shape of the encoded attributes\n",
    "print(\"Encoded Attributes Shape:\", encoded_attributes.shape)\n",
    "encoded_attributes = encoded_attributes.toarray() \n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Use PCA to reduce dimensions of the one-hot encoded attributes\n",
    "pca = PCA(n_components=128)  # Reduce to 128 features\n",
    "reduced_attributes = pca.fit_transform(encoded_attributes)\n",
    "\n",
    "print(\"Reduced Attributes Shape:\", reduced_attributes.shape)  # Verify shape\n",
    "\n",
    "\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# scaler = MinMaxScaler()\n",
    "# encoded_attributes = scaler.fit_transform(encoded_attributes)\n",
    "\n",
    "dataset = CombinedDataset(image_paths, reduced_attributes, labels, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a848bec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class HybridModel(nn.Module):\n",
    "    def __init__(self, num_image_features, num_attributes, num_classes):\n",
    "        super(HybridModel, self).__init__()\n",
    "        # CNN for images\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        # Fully connected layers\n",
    "        self.fc_image = nn.Linear(64 * 37 * 37, num_image_features)\n",
    "        # Embedding layer for attributes\n",
    "        self.fc_attributes = nn.Linear(num_attributes, 128)\n",
    "        self.fc_combined = nn.Linear(num_image_features + 128, 512)\n",
    "        self.fc_output = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, image, attributes):\n",
    "        # CNN forward\n",
    "        image_features = self.cnn(image)\n",
    "        image_features = self.fc_image(image_features)\n",
    "        # Process attributes\n",
    "        attributes_features = torch.relu(self.fc_attributes(attributes))\n",
    "        # Combine with attributes\n",
    "        combined_features = torch.cat((image_features, attributes_features), dim=1)\n",
    "        x = F.relu(self.fc_combined(combined_features))\n",
    "        x = self.fc_output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0913087d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w9/03c_s4jn509125_l99795s6c0000gn/T/ipykernel_5700/3424561718.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  images, attributes, labels = images.to(device), attributes.to(device), torch.tensor(labels).long().to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 5.2412, Accuracy: 1.35%\n",
      "Epoch 2/10, Loss: 5.0472, Accuracy: 2.80%\n",
      "Epoch 3/10, Loss: 4.8071, Accuracy: 4.56%\n",
      "Epoch 4/10, Loss: 4.5944, Accuracy: 6.75%\n",
      "Epoch 5/10, Loss: 4.3862, Accuracy: 8.76%\n",
      "Epoch 6/10, Loss: 4.1995, Accuracy: 11.74%\n",
      "Epoch 7/10, Loss: 4.0042, Accuracy: 13.45%\n",
      "Epoch 8/10, Loss: 3.7785, Accuracy: 17.22%\n",
      "Epoch 9/10, Loss: 3.5516, Accuracy: 20.45%\n",
      "Epoch 10/10, Loss: 3.3355, Accuracy: 24.83%\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "num_image_features = 128\n",
    "num_attributes = reduced_attributes.shape[1]\n",
    "num_classes = len(set(labels))\n",
    "model = HybridModel(num_image_features, num_attributes, num_classes).to(device)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, attributes, labels in dataloader:\n",
    "        images, attributes, labels = images.to(device), attributes.to(device), torch.tensor(labels).long().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images, attributes)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "    print(f\"Epoch {epoch + 1}/10, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d666f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.45%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Step 1: Define the transformations for the test data (same as for training)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize((150, 150)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Step 2: Load the test dataset using ImageFolder\n",
    "test_dir = 'data/test_images'  # Path to your test image directory\n",
    "\n",
    "test_dataset = datasets.ImageFolder(root=test_dir, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)  # No shuffle for evaluation\n",
    "\n",
    "# Step 3: Evaluate the model\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)  # Get class with highest score\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Validation Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73619ea9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
