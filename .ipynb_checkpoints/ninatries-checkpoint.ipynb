{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2735c935",
   "metadata": {
    "id": "2735c935"
   },
   "source": [
    "<h1><center> Group Assignment: Bird Classifier  </center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cd0b3ce3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 250
    },
    "executionInfo": {
     "elapsed": 224,
     "status": "error",
     "timestamp": 1694776892897,
     "user": {
      "displayName": "York Pudds (Yorkie)",
      "userId": "15802187727162843994"
     },
     "user_tz": -120
    },
    "id": "cd0b3ce3",
    "outputId": "da1cfd65-38c1-4971-dd05-c2a17ce22140"
   },
   "outputs": [],
   "source": [
    "# Import PyTorch and supporting libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "60e54e04",
   "metadata": {
    "id": "60e54e04",
    "outputId": "2dda9a07-4c8b-4e52-e8a0-744f58857013"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Use GPU if available, else fallback to CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7046355d-9c8e-4376-a2a4-02c98ad8f848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load in data sets and set variables\n",
    "\n",
    "# Load the CSV file\n",
    "# train data\n",
    "train_csv_path = \"C:/Users/Gebruiker/Documents/UVA/Vakken met code/AML/AML2024-Feather-in-Focus/data/train_images.csv\"\n",
    "train_label_df = pd.read_csv(train_csv_path)\n",
    "\n",
    "# test data\n",
    "test_csv_path = \"C:/Users/Gebruiker/Documents/UVA/Vakken met code/AML/AML2024-Feather-in-Focus/data/test_images_path.csv\"  # Update this path\n",
    "test_label_df = pd.read_csv(test_csv_path)\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0f24d0fe-b281-4f53-9043-fbe869ff0192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create a Custom Dataset\n",
    "\n",
    "class ImageLabelDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (str): Path to the CSV file containing image paths and labels.\n",
    "            transform (callable, optional): Transformations to apply to the images.\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the total number of samples\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the image path and label for the given index\n",
    "        img_path = \"./images/\"+ self.data.iloc[idx, 0]  # Image file path\n",
    "        label = self.data.iloc[idx, 1] if self.data.shape[1] > 1 else 0   # Corresponding label\n",
    "\n",
    "        # Load the image\n",
    "        image = Image.open(img_path).convert(\"RGB\")  # Ensure 3 channels (RGB)\n",
    "\n",
    "        # Apply transformations (if any)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Convert label to tensor\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "66cd1321-7c6d-4166-811e-e6e1673c34d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define Transformations\n",
    "\n",
    "# Define transformations (resize, convert to tensor, normalize)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize( (128, 128)),  # Resize to 128x128\n",
    "    transforms.ToTensor(),          # Convert to tensor\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize values\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e2019c04-f55b-4944-a719-f04d53bae382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Instantiate the Dataset and DataLoader\n",
    "\n",
    "# Instantiate the dataset\n",
    "train_dataset = ImageLabelDataset(csv_file=train_csv_path, transform=transform)\n",
    "test_dataset = ImageLabelDataset(csv_file=test_csv_path, transform=transform)\n",
    "\n",
    "# Create a DataLoader for batch processing\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False) # set batch size to 1 so that we check all 4000 images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3313f93c-48e1-49fe-a198-7d9e6fe88642",
   "metadata": {},
   "source": [
    "Run the below code to see the difference if you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7667ac47-65fe-47bf-9f86-d8e2ad973281",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"train dataset size: {len(train_dataset)}, train data size after data loader:  {len(ttrain_dataloader)}\")\n",
    "#print(f\"test dataset size: {len(test_dataset)}, test data size after data loader:  {len(test_dataloader)}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1a3ba616-c605-4716-a046-89b16aa78d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\Gebruiker\\Documents\\UVA\\Vakken met code\\AML\\AML2024-Feather-in-Focus\\data\n",
      "Current working directory: C:\\Users\\Gebruiker\\Documents\\UVA\\Vakken met code\\AML\\AML2024-Feather-in-Focus\\data\n"
     ]
    }
   ],
   "source": [
    "print(\"Current working directory:\", os.getcwd())\n",
    "# Change the working directory to the folder containing the files\n",
    "os.chdir(\"C:/Users/Gebruiker/Documents/UVA/Vakken met code/AML/AML2024-Feather-in-Focus/data\")\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ab990d05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch of images: torch.Size([32, 3, 128, 128])\n",
      "Batch of labels: tensor([ 21, 141, 168, 151, 105, 175, 182,  33, 110, 157,  60, 155,  10,  35,\n",
      "        121,   2,  18, 164,  90, 140,  28,  14, 118,  13,  20, 102, 135,  59,\n",
      "         76, 129,  71,  14])\n",
      "\n",
      "Batch of images: torch.Size([1, 3, 128, 128])\n",
      "Batch of labels: tensor([1])\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Verify the Dataset\n",
    "\n",
    "# Iterate through the train DataLoader\n",
    "for images, labels in train_dataloader:\n",
    "    print(f\"Batch of images: {images.shape}\")  # Should be (batch_size, channels, height, width)\n",
    "    print(f\"Batch of labels: {labels}\")       # Should match the batch_size\n",
    "    break\n",
    "\n",
    "# Iterate through the test DataLoader\n",
    "for images, labels in test_dataloader:\n",
    "    print(f\"\\nBatch of images: {images.shape}\")  # Should be (batch_size, channels, height, width)\n",
    "    print(f\"Batch of labels: {labels}\")       # Should match the batch_size\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "48e720ca-6fa0-4f5a-9938-913811d9e820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Create CNN model\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # Define layers\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)  # Input channels: 3, Output channels: 16\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1) # Input: 16, Output: 32\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)                  # Downsample by 2x\n",
    "        self.fc1 = nn.Linear(32 * 32 * 32, 128)                              # Fully connected layer\n",
    "        self.fc2 = nn.Linear(128, 200)                                     # Output layer (200 classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #print(f\"Input shape: {x.shape}\")  # Debugging input shape\n",
    "        x = self.pool(torch.relu(self.conv1(x)))  # Conv1 -> ReLU -> MaxPool\n",
    "        #print(f\"After Conv1: {x.shape}\")\n",
    "        x = self.pool(torch.relu(self.conv2(x)))  # Conv2 -> ReLU -> MaxPool\n",
    "        #print(f\"After Conv2: {x.shape}\")\n",
    "        x = x.view(x.size(0), -1)                # Flatten for fully connected layers\n",
    "        #print(f\"After Flatten: {x.shape}\")\n",
    "        x = torch.relu(self.fc1(x))              # FC1 -> ReLU\n",
    "        #print(f\"After Fully Connected: {x.shape}\")\n",
    "        x = self.fc2(x) # FC2 (logits output)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Instantiate the model and move it to the appropriate device\n",
    "model = CNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c3044cf1-7e4b-450f-9f0b-2cfa5bf9ced5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Set loss and optimiser function\n",
    "\n",
    "# Loss function: CrossEntropyLoss (good for classification tasks)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer: Adam optimizer with a learning rate\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8175d25b-72e4-4518-b23d-cac1287f16b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 5.2722\n",
      "Epoch [2/10], Loss: 5.0447\n",
      "Epoch [3/10], Loss: 4.7228\n",
      "Epoch [4/10], Loss: 4.2274\n",
      "Epoch [5/10], Loss: 3.3882\n",
      "Epoch [6/10], Loss: 2.2982\n",
      "Epoch [7/10], Loss: 1.2788\n",
      "Epoch [8/10], Loss: 0.5964\n",
      "Epoch [9/10], Loss: 0.2389\n",
      "Epoch [10/10], Loss: 0.0998\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Training loop\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, labels in train_dataloader:\n",
    "        # Move data to device\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels - 1) # to combat the out of bounds issue I set labels - 1 \n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss for display\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_dataloader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3674fe22-6331-474f-95c1-eddb09ea6355",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 9: Evaluation loop\n",
    "\n",
    "model.eval()  # Set model to evaluation mode\n",
    "predictions = []  # To store predictions\n",
    "count = 0 # this keeps track of which image we are currently predicting for\n",
    "\n",
    "with torch.no_grad():  # No need to compute gradients during evaluation\n",
    "    for images, labels in test_dataloader: \n",
    "        \n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)  # Get predicted class\n",
    "        predicted += 1 # to counteract the out of bounds issue, we need to do y+1\n",
    "\n",
    "        # get the current images' image path\n",
    "        image_path = test_label_df[\"image_path\"][count]\n",
    "\n",
    "        # Extract the file name without the extension\n",
    "        image_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "        \n",
    "        # Store the image path and prediction\n",
    "        predictions.append((image_name, predicted.item()))\n",
    "        count += 1\n",
    "\n",
    "# Save predictions to Excel\n",
    "df = pd.DataFrame(predictions, columns=['id', 'label'])\n",
    "df.to_csv(\"predictions.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8f086d-8c92-44a6-ab28-37e212c17dd2",
   "metadata": {},
   "source": [
    "Below is a parameter log with their kaggle results.\n",
    "\n",
    "The format will always be training batch size, learing rate and number of epochs, optimiser :-\n",
    "1. batch size = 32, learning rate = 0.001, number of epochs = 15, optimizer = ADAM, Kaggle score = 0.00525\n",
    "2. batch size = 32, learning rate = 0.01, number of epochs = 15, optimizer = ADAM, Kaggle score = 0.00500\n",
    "3. batch size = 32, learning rate = 0.01, number of epochs = 10, optimizer = ADAM, Kaggle score = 0.00600\n",
    "4. batch size = 32, learning rate = 0.01, number of epochs = 8, optimizer = ADAM, Kaggle score = 0.00525\n",
    "5. batch size = 4, learning rate = 0.01, number of epochs = 10, optimizer = SDG, Kaggle score = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37577d00-cf3f-42de-9849-78fe309b971c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
